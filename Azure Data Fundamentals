Chapter 1 - Data

How Data is defined and stored?
Identify characteristics of relational and non-relational data
Define and differentiate data workloads
Define and differentiate batch and streaming data

# Data -  
collection of facts, numbers, descriptions, objects, stored in a structured, semi-structured, unstructured way

* Structured data is basically tabular data i.e. represented by rows and columns in a database

* The mathematical term relation refers to an organised set of data held as a table.

* Semi structred data doesn't reside in a relational database but has some structure
    Examples include documents held in Javascript object edition or JSON format
    More Semi-structured database examples:
    1. Key-value stores - similar to relational table except that each row can have any number of columns
    2. Graph Databases - can be used to store and query information about complex relationships. A graph contains nodes, which is information about objects and edges which is about the relationship between objects
    etc....

* Unstructured data examples - Audio, Video files and binary data files

# Data processing solutions 
Often fall into one of two categories :
1. Analytical Systems - In contrast to system designed to support OLTP an Analytical system is designed to support business users who need to query and need a big picture view of the information held in a database.
    *   It is concerned with capturing raw Data and using it to generate insights. An organization can use these insights to make business decisions. 
        For Eg - Detailed insight for a manufacturing company might indicate trends enabling them to determine which product lines to focus on for profitability. 
    *   Most analytical systems need to perform similar tasks like Data injection, data transformation, Data Querying and Data Visualization.

       
2. Transactional processsing systems -  often what most people consider the primary function of business computing. It records transactions which could be financial, movement of money between two accounts in a Banking System. OR it might be a part of retail system such as tracking payments for goods and services from customers.
    
    Transaction is nothing but a small discrete unit of work and transactional systems are often high volume sometimes handling millions of transactions in a single day. 

    The data being processed has to accessible very quickly. The work done by transactional systems is referred to as OLTP (Online transactional processing).  To support fast processing, the data in a transactional system is divided into small pieces.
        For example - if we are using a relational system, each table involved in a transaction only contains the columns necessary to perform the transactional task and normalization can enable a transactional system to cache much of the info required to perform transaction in memory and speed throughput. 


# Components in a typical Data Processing System :
    Some Sources of Data : On Premise Data(SQL Server, Oracle, Fileshares, SAP), Cloud Data (Azure, AWS, GCP), SAAS Data (Salesforce, Dynamics)

      Raw data is extracted from diffrent sources like Healthcare Data (Critical Device    Data), Financial transactions and Point-Of-Sale Data and stored in a database/file storage. Then data is transformed and cleansed for Data Visualizations and Data Queries  
       
       1. Data Ingestion - process of capturing raw data. 
            Data could be taken from control devices measuring environmental info such as temp and pressure. 
            OR 
            Point-Of-Sale devices recording the items purchased by a customer in a supermarket.
            OR
            Financial Data recording the movement of money between Bank accounts 
            OR
            Weather data from weather stations
            OR
            Some of this data might come from a separate OLTP system as well. 
             
        2. Data Storage - To process and analyze this Data you must first store the data in a repository of some sort. The repository could be a File Store, a Document Database or even a Relational Database and you can keep the data in Azure SQL Database or any other storage platform.
        
        3. Data Processing - Raw data is usually in a format that is not suitable for querying, the data might contain anamalies that should be filtered out OR it may require some transforming in some way. 
        After data ingestion you may want to do some cleaning operations and remove any questionable or invalid data.


        4. Data Visualization -   can be used as a tool for examining data. You can generate charts such as bar charts, line charts, plot results or geographical maps, PI charts or illustrate how data changes over time.

        Microsoft offers Data Visualization tools like PowerBI to provide rich graphical representation of your data. 

# Data processing
    is simply the conversion of raw data to meaningful info through a process.
Depending on how the data is ingested on your system Data processing is divided into two methods: 

*   Batch Processing/Data :  Buffering Raw Data and processing in groups
            * With this method, newly arriving data elements are collected in to a group. The whole group is then processed at a future time as a batch.
            * Exactly WHEN? each group is processed can be determined in the number of ways.
                For example - a) Processing Data at a scheduled time interval 
                            b) Processing when a certain amount of data has arrived
                            or as a result of some other event 
            
        An example of Batch Processing is a way that Credit Card Companies handle billing - They collect Monthly Data and generate bill which is sent to the consumer instead of sending bills for each purchase 

        Advantages : *  Last volumes of Data can be processed at a convenient time.
                 *  Can be schduled to run at time when systems or computers might otherwise be idle such as overnight or off-peak hours
    
        Disadvantages: Time Delay between ingesting the data and gaining the results.

*   Streaming Processing/Data :  processing each data as it arrives.
    
        **  Data injection is inherently a streaming process and streaming handles data in real time. 
        * Unlike batch processing there is no waiting untill the next batch processig interval.
        * Data is processed as individual pieces rather than being processed as a batch at a time
        * Beneficial in most scenerios where new dynamic data is generated on a continual basis.

        Examples:
            1. A financial info tracks changes in the stock market in real time
            2. Online gaming company lets real time data about player game interactions
            3. A real estate website that tracks a subset of data from customers mobile devices and makes real time property recommendations for properties to visit based on their Geo location

*   Apart from the way in which Batch Processing and Streaming Processing handle data, there are other differences as well.
        1. Data Scope : 
            Batch Data can process all the data in the data set. 
            
            Stream processing typically only has accces to the most recent data received or within a rolling time window, the last 30 secs for eg.

        2. Data Size :
            Batch data is suitable for handling large data set efficiently
            
            Stream processing is intended for individual records or micro batches consisting few records
        
        3. Performance : 
            The latency for batch processing is typically a few hours

            Stream processing occurs almost immediately with latency in the order of seconds or miliseconds 
        Latency is the time taken for the data to be received and processed

        4. Analysis:
            Batch processing is usually used for performing complex analytics
            
            Stream is used for simple response fucntions, aggregates or calculations such as rolling averages


# Chapter 2 - Roles and Responsibilities in the word of Data

    Data Job Roles
    Common Tasks and Tools for Data job roles

* Data Job Roles: 
    1. Database Administrator: 
        For Example - Azure Databasde Admin is responsible for the design, implementation, maintainance and operational aspect of on premises and cloud based database solutions built on Azure Data Services and SQL Server. 
        They are responsible for the overall availability and consistent performance and optimization of the database solutions. 
        They work with the stakeholders to  implement policies, tools and processes for backup and trecovery plans.
        DBA is also responsible for managing the data security in database. Granting privilages over the data and granting ordinary access to users as appropriate. 

    2. Data Engineer :
        Collaborates with stakeholders to design and implement data related assets and inlude data ingestion pipelines, cleansing and transformational activities and data stores for analytical workloads.
        They use a wide range of Data platform techs including relational and non-relational databases for file stores and Data streams.
        Ensuring that privacy of data is maintained within the cloud and spanning from on-premises to the cloud data store.
        Own the management and monitoring of the data stores and data pipelines to ensure that data loads perform as expected.

    3. Data Analyst / Scientist:
        Enables business to maximise the value of Data assets.
        They are responsible for designing and building scalable models, cleansing and transforming Data and enabling advanced analytic capabilities through reports and visualizations
        Processes raw data into relevant insight based on identified business requirement to deliver relevant insights/solutions


Common Tasks and Tools for Data job roles:
    
    *   Common Tools for DBA - Most DB systems provide their own set of tools to assist with DB administration.
            
            For Example :
            SQL Server DBA use SQL Server Management Studio (SSMS)
        
            Other Systems have their own specific interfaces as well such as PG Admin for POSTGRE SQL Systems or MySQL Workbench for MYSQL DB.

            There are also a number of cross platform DB Admin tools like Azure Data Studio

        *   Azure Data Studio:
        
            graphical interface for managing on-premise and cloud-based data services
            Runs on Windows, MacOS, Linux

                Provides a graphical user interface for managing manay different DB Systems
                It currently provides connections to on-premises SQL Server DBs, Azure SQL DB, PSTGRE SQL   Azure SQl DWH and SQL Server Big Data Clusters among others
                you can dwnld and install extensions from 3rd party developers that connect to other systems 
                OR provide results that help to automate mnany admin tasks  
        
        *   SQL Server Management Studio (SSMS): 
        
            graphical interface for managing on-premise and cloud-based data services
            Runs on Windows
            Comprehensive DB Admin tool

                Provides a  graphical interface enabling you to query Data, 
                Perform general Database Admin Tasks and 
                Generate Scripts for automating DB maintainance and support operations 
                
                Can be used to Backup a Database
                
                A useful SSMS is ability to generate Transact-SQL(T-SQL) script for all the functionality that SSMS provides - This gives DBA the ability schedule and automate many common tasks

        *   Azure Portal to manage Azure SQL DB:

            Manual and automation of scripts using Azure Resource Manager or Command Line Interface Scripting

                Azure SQL DB provides Db services in Azure.
                Similar to SQL Server except that it runs in the cloud
                Typical configuration tasks such as increasing the DB size, creating a new DB and deleting an existing DB are done using Azure Portal
                Can be used to dynamically manage and adjust resources such as Data Storage size and number of cores available for Database processing. These tasks would require the support of a System Admin if you are running the DB on-premises.  

    *   Common tools for DB Engineer
            For Example: 
            Azure Synapse Studio - Azure Portal integrated to manage Azure Synapse
                                    Data ingestion (Azure Data Factory)
                                    Management of Azure Synapse assets (SQL Pools/Spark Pool)

        To master Data Engineering you will need to be :
            1. familiar with a range of tools that enable you to create a well-designed DB, optimized for the business processes that will be run
            2. thorough understanding of architecture of DB manmagement system
            3. platform on which the system runs
            4. business requirements for the Data being stored in DB

        A.  If you are using relational DB management system, you need to be fluent in SQL
                Use Sql to create DBs, tables, indexes, views and other objects required by the DB
            
        B.  In some cases you may need to interact with the DB from a command line
                Many DBMS provide a Command Line Interface (CLI) that supports these operations.
                    For Eg - use SQL CMD Utility to connect to the Microsoft SQLServer and Azure SQL DB and run a talk queries and commands  
                 
    *   Common tools - Data Analyst
        
        - Data Analysts explore the Data and use it to determine trends, issues and gain other insights and that might be of benefit to the company.
        - A large part of the Data Analyst role is concerned with communication and visualization
        - Data visualization is a key to presenting large amounts of info in ways that are universally understable or easy to interpret and spot patterns, trends and co-relations.
        - These representations include Charts, Graphs, Infographics and other pictorial diagrams
        - Reporting tools are required to be a good Data Analyst like Microsoft Power BI and SSRS (SQL Server  Reporting Services)

        Examples :- 
            Power BI Desktop : Data Visualization tool
                                Model and visualize data
                                Management of Azure Synapse assets (SQL Pools/Spark Pool)

            Power BI Portal/Power BI Service : Authoring and management of Power BI reports 
                                                Authoring of Power BI dashboards
                                                Share reports/datasets

            Power BI Report Builder : Data Visualization tool for paginated reports
                                        Model and Visualize paginated reports 


# Chapter 3 - Describe concepts of relational Data

Exploring the characrteristics of Relational Data
Define tables, indexes and views
Explore relational Data workload offerings in Azure
